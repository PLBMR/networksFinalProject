logLike = 0
#sum over number of observations
for (obsNum in 1:dim(data)[1]){
givenObs = data[obsNum,]
#sum over classes for the mixture
givenObsLikelihood = 0 #we will add to this
for (class in 1:length(classProbVec)){
classMix = (classProbVec[class]
* lieExamMatrix[class,givenObs$LIEEXAM]
* liePaperMatrix[class,givenObs$LIEPAPER]
* fraudMatrix[class,givenObs$FRAUD]
* copyExamMatrix[class,givenObs$COPYEXAM])
givenObsLikelihood = givenObsLikelihood + classMix
}
logLike = logLike + log(givenObsLikelihood)
}
return(logLike)
}
#our wrapper for such
logLike.latentClassMod <- function(data,givenLCM){
logLike = dLatentClassMod(data,givenLCM,log = TRUE)
return(logLike)
}
logLike.latentClassMod(cheating,cheatingMod.lcm)
logLike.latentClassMod(cheating,cheatingMod.lcm)
#main function for calculating our data's loglikelihoods
dLatentClassMod <- function(data,givenLCM,log = FALSE){
#first get our class probabilities
classProbVec = givenLCM$P
#get matrices of conditional probabilities
lieExamMatrix = givenLCM$probs$LIEEXAM
liePaperMatrix = givenLCM$probs$LIEPAPER
fraudMatrix = givenLCM$probs$FRAUD
copyExamMatrix = givenLCM$probs$COPYEXAM
#then build log likelihood
logLike = 0
#sum over number of observations
for (obsNum in 1:dim(data)[1]){
givenObs = data[obsNum,]
#sum over classes for the mixture
givenObsLikelihood = 0 #we will add to this
for (class in 1:length(classProbVec)){
classMix = (classProbVec[class]
* lieExamMatrix[class,givenObs$LIEEXAM]
* liePaperMatrix[class,givenObs$LIEPAPER]
* fraudMatrix[class,givenObs$FRAUD]
* copyExamMatrix[class,givenObs$COPYEXAM])
givenObsLikelihood = givenObsLikelihood + classMix
}
logLike = logLike + log(givenObsLikelihood)
}
return(logLike)
}
#our wrapper for such
logLike.latentClassMod <- function(data,givenLCM){
logLike = dLatentClassMod(data,givenLCM,log = TRUE)
return(logLike)
}
#load in the cross validation code
cv.logLikeLCM <- function(data,fold.labels,numClusters,nfolds = 5) {
logLikeVec = rep(0,nfolds) #we will build on this
for (fold in 1:nfolds) {
test.rows <- which(fold.labels == fold)
train <- data[-test.rows, ]
test <- data[test.rows, ]
current.model.lcm <- poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = train,nclass = numClusters,
verbose = FALSE)
logLikeVec[fold] = logLike.latentClassMod(test,current.model.lcm)
}
avgLogLike = sum(logLikeVec) / length(logLikeVec)
return(avgLogLike)
}
sample(1,2,3,)
sample(1,2,3,)
sample(1,2,3)
sample(1,2,3)
sample(c(1,2,3))
sample(rep(1:5,length.out = 30))
sample(rep(1:5,length.out = 30))
sample(rep(1:5,length.out = 30))
sample(rep(1:5,length.out = 30))
?sample
length(sample(rep(1:5,length.out = 30)))
candidateClusterVec = 2:9
candidateClusterVec
#first generate a set of fold labels for 5-fold cv
fold.labels <- sample(rep(1:nfolds, length.out = n))
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 9
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,lenght(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters] = cv.logLikeLCM(cheating,
fold.labels,k)
}
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters] = cv.logLikeLCM(cheating,
fold.labels,k)
}
print(avgLogLikelihoodVec)
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
print(avgLogLikelihoodVec)
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 9
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
print(avgLogLikelihoodVec)
dLatentClassMod <- function(data,givenLCM,log = FALSE){
#first get our class probabilities
classProbVec = givenLCM$P
#get matrices of conditional probabilities
lieExamMatrix = givenLCM$probs$LIEEXAM
liePaperMatrix = givenLCM$probs$LIEPAPER
fraudMatrix = givenLCM$probs$FRAUD
copyExamMatrix = givenLCM$probs$COPYEXAM
#then build log likelihood
logLike = 0
#sum over number of observations
for (obsNum in 1:dim(data)[1]){
givenObs = data[obsNum,]
#sum over classes for the mixture
givenObsLikelihood = 0 #we will add to this
for (class in 1:length(classProbVec)){
classMix = (classProbVec[class]
* lieExamMatrix[class,givenObs$LIEEXAM]
* liePaperMatrix[class,givenObs$LIEPAPER]
* fraudMatrix[class,givenObs$FRAUD]
* copyExamMatrix[class,givenObs$COPYEXAM])
givenObsLikelihood = givenObsLikelihood + classMix
}
logLike = logLike + log(givenObsLikelihood)
}
print(length(classProbVec))
print(logLike)
return(logLike)
}
#our wrapper for such
logLike.latentClassMod <- function(data,givenLCM){
logLike = dLatentClassMod(data,givenLCM,log = TRUE)
return(logLike)
}
cv.logLikeLCM <- function(data,fold.labels,numClusters,nfolds = 5) {
logLikeVec = rep(0,nfolds) #we will build on this
for (fold in 1:nfolds) {
test.rows <- which(fold.labels == fold)
train <- data[-test.rows, ]
test <- data[test.rows, ]
current.model.lcm <- poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = train,nclass = numClusters,
verbose = FALSE)
logLikeVec[fold] = logLike.latentClassMod(test,current.model.lcm)
}
avgLogLike = sum(logLikeVec) / length(logLikeVec)
return(avgLogLike)
}
```
```{r,performCrossValidation}
#first generate a set of fold labels for 5-fold cv
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 9
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
print(avgLogLikelihoodVec)
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 9
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
#main function for calculating our data's loglikelihoods
dLatentClassMod <- function(data,givenLCM,log = FALSE){
#first get our class probabilities
classProbVec = givenLCM$P
#get matrices of conditional probabilities
lieExamMatrix = givenLCM$probs$LIEEXAM
liePaperMatrix = givenLCM$probs$LIEPAPER
fraudMatrix = givenLCM$probs$FRAUD
copyExamMatrix = givenLCM$probs$COPYEXAM
#then build log likelihood
logLike = 0
#sum over number of observations
for (obsNum in 1:dim(data)[1]){
givenObs = data[obsNum,]
#sum over classes for the mixture
givenObsLikelihood = 0 #we will add to this
for (class in 1:length(classProbVec)){
classMix = (classProbVec[class]
* lieExamMatrix[class,givenObs$LIEEXAM]
* liePaperMatrix[class,givenObs$LIEPAPER]
* fraudMatrix[class,givenObs$FRAUD]
* copyExamMatrix[class,givenObs$COPYEXAM])
givenObsLikelihood = givenObsLikelihood + classMix
}
logLike = logLike + log(givenObsLikelihood)
}
return(logLike)
}
#our wrapper for such
logLike.latentClassMod <- function(data,givenLCM){
logLike = dLatentClassMod(data,givenLCM,log = TRUE)
return(logLike)
}
#load in the cross validation code
cv.logLikeLCM <- function(data,fold.labels,numClusters,nfolds = 5) {
logLikeVec = rep(0,nfolds) #we will build on this
for (fold in 1:nfolds) {
test.rows <- which(fold.labels == fold)
train <- data[-test.rows, ]
test <- data[test.rows, ]
current.model.lcm <- poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = train,nclass = numClusters,
verbose = FALSE)
logLikeVec[fold] = logLike.latentClassMod(test,current.model.lcm)
}
avgLogLike = sum(logLikeVec) / length(logLikeVec)
return(avgLogLike)
}
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 9
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
print(avgLogLikelihoodVec)
unique(fold.labels)
length(fold.labels[fold.labels == 1])
length(fold.labels[fold.labels == 2])
length(fold.labels[fold.labels == 3])
length(fold.labels[fold.labels == 4])
length(fold.labels[fold.labels == 5])
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1]
trainSet = cheating[fold.labels == 1,]
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
#get vector of log likelihoods
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
trainedMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = trainSet,nclass = k,verbose = FALSE)
anotherLogLikelihoodVec[k+1 - minClusters] = logLike.latentClassMod(
testSet,trainedMod.lcm)
}
anotherLogLikelihoodVec
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
#get vector of log likelihoods
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
trainedMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = trainSet,nclass = k,verbose = FALSE)
anotherLogLikelihoodVec[k+1 - minClusters] = logLike.latentClassMod(
testSet,trainedMod.lcm)
}
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
#get vector of log likelihoods
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
trainedMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = trainSet,nclass = k,verbose = FALSE)
anotherLogLikelihoodVec[k+1 - minClusters] = logLike.latentClassMod(
testSet,trainedMod.lcm)
}
anotherLogLikelihoodVec
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
#get vector of log likelihoods
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
trainedMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = trainSet,nclass = k,verbose = FALSE)
anotherLogLikelihoodVec[k+1 - minClusters] = logLike.latentClassMod(
testSet,trainedMod.lcm)
}
anotherLogLikelihoodVec
anotherLogLikelihoodVec
anotherLogLikelihoodVec
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
#get vector of log likelihoods
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
trainedMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = trainSet,nclass = k,verbose = FALSE)
anotherLogLikelihoodVec[k+1 - minClusters] = logLike.latentClassMod(
testSet,trainedMod.lcm)
}
anotherLogLikelihoodVec
dLatentClassMod <- function(data,givenLCM,log = FALSE){
#first get our class probabilities
classProbVec = givenLCM$P
#get matrices of conditional probabilities
lieExamMatrix = givenLCM$probs$LIEEXAM
liePaperMatrix = givenLCM$probs$LIEPAPER
fraudMatrix = givenLCM$probs$FRAUD
copyExamMatrix = givenLCM$probs$COPYEXAM
#then build log likelihood
logLike = 0
#sum over number of observations
for (obsNum in 1:dim(data)[1]){
givenObs = data[obsNum,]
#sum over classes for the mixture
givenObsLikelihood = 0 #we will add to this
for (class in 1:length(classProbVec)){
classMix = (classProbVec[class]
* lieExamMatrix[class,givenObs$LIEEXAM]
* liePaperMatrix[class,givenObs$LIEPAPER]
* fraudMatrix[class,givenObs$FRAUD]
* copyExamMatrix[class,givenObs$COPYEXAM])
print(classMix)
givenObsLikelihood = givenObsLikelihood + classMix
}
logLike = logLike + log(givenObsLikelihood)
}
return(logLike)
}
logLike.latentClassMod <- function(data,givenLCM){
logLike = dLatentClassMod(data,givenLCM,log = TRUE)
return(logLike)
}
numFolds = 2
fold.labels <- sample(rep(1:numFolds,length.out = dim(cheating)[1]))
trainSet = cheating[fold.labels == 1,]
testSet = cheating[fold.labels == 2,]
#get vector of log likelihoods
anotherLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
trainedMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = trainSet,nclass = k,verbose = FALSE)
anotherLogLikelihoodVec[k+1 - minClusters] = logLike.latentClassMod(
testSet,trainedMod.lcm)
}
anotherLogLikelihoodVec
cheatingMod.lcm
newCheatingMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,data = cheating,nclass = 3,verbose = FALSE)
newCheatingMod.lcm
newCheatingMod.lcm = poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,data = cheating,nclass = 4,verbose = FALSE)
newCheatingMod.lcm
logLike.latentClassMod(cheating,newCheatingMod.lcm)
newCheatingMod.lcm
#load in the cross validation code
cv.logLikeLCM <- function(data,fold.labels,numClusters,nfolds = 5) {
logLikeVec = rep(0,nfolds) #we will build on this
for (fold in 1:nfolds) {
test.rows <- which(fold.labels == fold)
train <- data[-test.rows, ]
test <- data[test.rows, ]
current.model.lcm <- poLCA(cbind(LIEEXAM,LIEPAPER,FRAUD,COPYEXAM)~1,
data = train,nclass = numClusters,
verbose = FALSE)
logLikeVec[fold] = logLike.latentClassMod(test,current.model.lcm)
}
avgLogLike = sum(logLikeVec) / length(logLikeVec)
return(avgLogLike)
}
dLatentClassMod <- function(data,givenLCM,log = FALSE){
#first get our class probabilities
classProbVec = givenLCM$P
#get matrices of conditional probabilities
lieExamMatrix = givenLCM$probs$LIEEXAM
liePaperMatrix = givenLCM$probs$LIEPAPER
fraudMatrix = givenLCM$probs$FRAUD
copyExamMatrix = givenLCM$probs$COPYEXAM
#then build log likelihood
logLike = 0
#sum over number of observations
for (obsNum in 1:dim(data)[1]){
givenObs = data[obsNum,]
#sum over classes for the mixture
givenObsLikelihood = 0 #we will add to this
for (class in 1:length(classProbVec)){
classMix = (classProbVec[class]
* lieExamMatrix[class,givenObs$LIEEXAM]
* liePaperMatrix[class,givenObs$LIEPAPER]
* fraudMatrix[class,givenObs$FRAUD]
* copyExamMatrix[class,givenObs$COPYEXAM])
givenObsLikelihood = givenObsLikelihood + classMix
}
logLike = logLike + log(givenObsLikelihood)
}
return(logLike)
}
#our wrapper for such
logLike.latentClassMod <- function(data,givenLCM){
logLike = dLatentClassMod(data,givenLCM,log = TRUE)
return(logLike)
}
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 9
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
avgLogLikelihoodVec
numFolds = 5
fold.labels <- sample(rep(1:numFolds, length.out = dim(cheating)[1]))
#create candidate set of clusters
minClusters = 2
maxClusters = 18
candidateClusterVec = minClusters:maxClusters
#then get the vector of log likelihoods
avgLogLikelihoodVec = rep(0,length(candidateClusterVec))
for (k in candidateClusterVec){
avgLogLikelihoodVec[k - minClusters + 1] = cv.logLikeLCM(cheating,
fold.labels,k)
}
avgLogLikelihoodVec
plot(candidateClusterVec,avgLogLikelihoodVec,
main = "Average Log Likelihood\nGiven Number of Classes",
ylab = "Average Log Likelihood",
xlab = "Number of Classes",pch = pchVal,
bg = "Black")
?cheating
?list
?matrix
a = c(3,4,6)
a[1] = "A"
a
?data.frame
?rbind
?apply
?sapply
?diag
install.packages("sand")
library(devtools)
install.packages("devtools")
install_github("sigopt/SigOptR")
library(devtools)
install.packages("devtools")
library(devtools)
install.packages(c("mgcv", "nlme", "survival"), lib="/usr/local/Cellar/r/3.3.0/R.framework/Versions/3.3/Resources/library")
install.packages("devtools")
install.packages("devtools", lib="/usr/local/Cellar/r/3.3.0/R.framework/Versions/3.3/Resources/library")
install.packages(c('repr', 'IRdisplay', 'crayon', 'pbdZMQ', 'devtools'))
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()  # to register the kernel in the current R installation
install.packages(c("numDeriv", "RcppArmadillo", "tibble"))
print("Moo!!!!")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Box Sync/CMU/Fourth Year/f16/73-449_Social_Economic_And_Information_Networks/projects/networksFinalProject/code")
library(igraph)
library(ggplot2)
library(ggnet)
#load in dataset
givenGraph = read_graph("../data/processed/ukraineNetwork.gml",
format = "gml")
ggnet2(givenGraph)
ggnet2(givenGraph,node.size = nodeSize)
nodeSize = 2
ggnet2(givenGraph,node.size = nodeSize)
mpg
idea = colnames(mpg)
idea
